The iRL average return is slightly higher than the standard PPO policy.
This is because the iRL expert data was collected as the last 20 epochs of a 250 epoch run

- The iRL plot is a bit unstable because a higher LR was used compared to the standard iRL (Actually, no. Ignore)


- For the data collection policy. In the read me, remember to note

  >	Use "python3 collect_data.py --help" for a full list of all expert policy options available


- Mention use of soft ppo in airl. It improves returns significantly. Have a plot of airl without entropy regularization(soft ppo disabled) vs airl with it i.e soft ppo.

Also, a regularization temperature of .1 has no difference with that of .9 on AIRL averaged returns.

> [I actually suspect it's cz I was doing the adv normalization wrong and not sofr-ppo. Confirm this] - Yeah, it had to do with the wrong normalization. With and without entropy regularzn work equally well.


- Final version (in folder 4) is the best version (highest return). Can use this to note that the learned reward function is dependant on the quality of demonstrations.
No aiRL done on this run yet but expert plot data saved. -> [Update] It doesn't perform as well.





From ad_irl.py 
---------
Big Qs:
    1. How g(s) + h(s') - h(s) recovers the reward function f(s, a, s')
    without being a function of the action (state only)


next
airl: use lr 1e-4 both, use b1000 (Does bad)






ReadME
airl

- Include 'collect expert data from [here](link to g_drive)
